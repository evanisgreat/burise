{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Data\n",
    "\n",
    "We’ll simulate 100 observations from a Normal(μ=5, σ²=4) and import what we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  5.,  7., 13., 17., 18., 16., 11.,  7.,  5.]),\n",
       " array([-0.10597963,  0.85856926,  1.82311814,  2.78766703,  3.75221592,\n",
       "         4.71676481,  5.6813137 ,  6.64586258,  7.61041147,  8.57496036,\n",
       "         9.53950925]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGdCAYAAAAFcOm4AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIddJREFUeJzt3Xlw1PX9x/HXQmSDTrI0aJJdCBCYKHIUMZzhLiUYkEpFwQvi0GoZUYEMLYnHCM7IgtcwGITBckipQDsRiA22hAqJFFSOhFpFDGMgKSbNYGUX8MeG4/v7w3HrmkMXdlk/m+dj5jvj97vf7zfv3XHY53zzza7NsixLAAAABmsV6QEAAACuFEEDAACMR9AAAADjETQAAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgxkR4gVC5duqTPP/9ccXFxstlskR4HAAD8AJZl6fTp03K5XGrV6vKvs0RN0Hz++edKSUmJ9BgAAOAyVFdXq2PHjpd9fNQETVxcnKSvX5D4+PgITwMAAH4Ir9erlJQU//v45YqaoPnm10zx8fEEDQAAhrnS20W4KRgAABiPoAEAAMYjaAAAgPEIGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABiPoAEAAMaLifQAAPBDdMktivQIQTu2aHykRwBaDK7QAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjBR00paWlmjBhglwul2w2m7Zs2RLwuM1ma3R54YUXmjzn2rVrGz3m3LlzQT8hAADQ8gQdNGfPnlWfPn2Un5/f6OM1NTUBy+rVq2Wz2TRp0qRmzxsfH9/g2NjY2GDHAwAALVDQn0OTlZWlrKysJh9PTk4OWN+6datGjRqlrl27Nntem83W4FgAAIAfIqz30PznP/9RUVGRfvWrX33vvmfOnFHnzp3VsWNH3X777SorK2t2f5/PJ6/XG7AAAICWKaxB8/rrrysuLk533nlns/t1795da9euVWFhoTZs2KDY2FgNGTJEFRUVTR7jdrvlcDj8S0pKSqjHBwAAhghr0KxevVr333//994LM2jQID3wwAPq06ePhg0bpj/96U+68cYb9corrzR5TF5enjwej3+prq4O9fgAAMAQYfsup3fffVdHjhzRpk2bgj62VatW6t+/f7NXaOx2u+x2+5WMCAAAokTYrtCsWrVK6enp6tOnT9DHWpal8vJyOZ3OMEwGAACiTdBXaM6cOaOjR4/61ysrK1VeXq6EhAR16tRJkuT1evXnP/9ZL730UqPnmDZtmjp06CC32y1JWrBggQYNGqS0tDR5vV4tXbpU5eXlWrZs2eU8JwAA0MIEHTT79+/XqFGj/Os5OTmSpOzsbK1du1aStHHjRlmWpXvvvbfRc1RVValVq/9dHDp16pQefvhh1dbWyuFwqG/fviotLdWAAQOCHQ8AALRANsuyrEgPEQper1cOh0Mej0fx8fGRHgdAiHXJLYr0CEE7tmh8pEcAfvRC9f7NdzkBAADjETQAAMB4BA0AADBe2D6HBsCPl4n3owBAc7hCAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjETQAAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjxUR6AACIVl1yiyI9QtCOLRof6RGAy8IVGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPEIGgAAYDyCBgAAGI+gAQAAxgs6aEpLSzVhwgS5XC7ZbDZt2bIl4PEHH3xQNpstYBk0aND3nregoEA9evSQ3W5Xjx49tHnz5mBHAwAALVTQQXP27Fn16dNH+fn5Te5z2223qaamxr9s27at2XPu3btXU6ZM0dSpU3Xo0CFNnTpVkydP1vvvvx/seAAAoAWKCfaArKwsZWVlNbuP3W5XcnLyDz7nkiVLNGbMGOXl5UmS8vLyVFJSoiVLlmjDhg3BjggAAFqYsNxDs2vXLiUmJurGG2/UQw89pLq6umb337t3rzIzMwO2jR07Vnv27GnyGJ/PJ6/XG7AAAICWKeRBk5WVpT/+8Y9655139NJLL2nfvn362c9+Jp/P1+QxtbW1SkpKCtiWlJSk2traJo9xu91yOBz+JSUlJWTPAQAAmCXoXzl9nylTpvj/u1evXurXr586d+6soqIi3XnnnU0eZ7PZAtYty2qw7dvy8vKUk5PjX/d6vUQNAAAtVMiD5rucTqc6d+6sioqKJvdJTk5ucDWmrq6uwVWbb7Pb7bLb7SGbEwAAmCvsn0PzxRdfqLq6Wk6ns8l9Bg8erOLi4oBt27dvV0ZGRrjHAwAAUSDoKzRnzpzR0aNH/euVlZUqLy9XQkKCEhISNH/+fE2aNElOp1PHjh3TE088oeuvv16//OUv/cdMmzZNHTp0kNvtliTNmjVLw4cP1+LFi3XHHXdo69at2rFjh3bv3h2CpwgAAKJd0EGzf/9+jRo1yr/+zX0s2dnZWr58uT788EOtW7dOp06dktPp1KhRo7Rp0ybFxcX5j6mqqlKrVv+7OJSRkaGNGzfqqaee0tNPP61u3bpp06ZNGjhw4JU8NwAA0ELYLMuyIj1EKHi9XjkcDnk8HsXHx0d6HOBHrUtuUaRHwI/UsUXjIz0CWphQvX/zXU4AAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjETQAAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjETQAAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA48VEegDAdF1yiyI9AgC0eFyhAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGCzpoSktLNWHCBLlcLtlsNm3ZssX/2Pnz5zVv3jz17t1b1113nVwul6ZNm6bPP/+82XOuXbtWNputwXLu3LmgnxAAAGh5gg6as2fPqk+fPsrPz2/w2FdffaWDBw/q6aef1sGDB/Xmm2/q008/1S9+8YvvPW98fLxqamoCltjY2GDHAwAALVDQn0OTlZWlrKysRh9zOBwqLi4O2PbKK69owIABqqqqUqdOnZo8r81mU3JycrDjAAAAhP8eGo/HI5vNpnbt2jW735kzZ9S5c2d17NhRt99+u8rKyprd3+fzyev1BiwAAKBlCmvQnDt3Trm5ubrvvvsUHx/f5H7du3fX2rVrVVhYqA0bNig2NlZDhgxRRUVFk8e43W45HA7/kpKSEo6nAAAADGCzLMu67INtNm3evFkTJ05s8Nj58+d19913q6qqSrt27Wo2aL7r0qVLuvXWWzV8+HAtXbq00X18Pp98Pp9/3ev1KiUlRR6PJ6ifBVwpvvoA0eTYovGRHgEtjNfrlcPhuOL377B8l9P58+c1efJkVVZW6p133gl6wFatWql///7NXqGx2+2y2+1XOioAAIgCIf+V0zcxU1FRoR07dqh9+/ZBn8OyLJWXl8vpdIZ6PAAAEIWCvkJz5swZHT161L9eWVmp8vJyJSQkyOVy6a677tLBgwf1l7/8RRcvXlRtba0kKSEhQW3atJEkTZs2TR06dJDb7ZYkLViwQIMGDVJaWpq8Xq+WLl2q8vJyLVu2LBTPEQAARLmgg2b//v0aNWqUfz0nJ0eSlJ2drfnz56uwsFCSdMsttwQct3PnTo0cOVKSVFVVpVat/ndx6NSpU3r44YdVW1srh8Ohvn37qrS0VAMGDAh2PAAA0AJd0U3BPyahuqkICBY3BSOacFMwrrZQvX/zXU4AAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjETQAAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwXkykBwAA/Hh0yS2K9AhBO7ZofKRHwI8AV2gAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPEIGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABgv6KApLS3VhAkT5HK5ZLPZtGXLloDHLcvS/Pnz5XK51LZtW40cOVIfffTR9563oKBAPXr0kN1uV48ePbR58+ZgRwMAAC1U0EFz9uxZ9enTR/n5+Y0+/vzzz+vll19Wfn6+9u3bp+TkZI0ZM0anT59u8px79+7VlClTNHXqVB06dEhTp07V5MmT9f777wc7HgAAaIFslmVZl32wzabNmzdr4sSJkr6+OuNyuTR79mzNmzdPkuTz+ZSUlKTFixfrN7/5TaPnmTJlirxer95++23/tttuu00/+clPtGHDhh80i9frlcPhkMfjUXx8/OU+JSBoXXKLIj0C0KIdWzQ+0iPgCoTq/Tuk99BUVlaqtrZWmZmZ/m12u10jRozQnj17mjxu7969AcdI0tixY5s9xufzyev1BiwAAKBlCmnQ1NbWSpKSkpICticlJfkfa+q4YI9xu91yOBz+JSUl5QomBwAAJgvLXznZbLaAdcuyGmy70mPy8vLk8Xj8S3V19eUPDAAAjBYTypMlJydL+vqKi9Pp9G+vq6trcAXmu8d992rM9x1jt9tlt9uvcGIAABANQnqFJjU1VcnJySouLvZvq6+vV0lJiTIyMpo8bvDgwQHHSNL27dubPQYAAOAbQV+hOXPmjI4ePepfr6ysVHl5uRISEtSpUyfNnj1bCxcuVFpamtLS0rRw4UJde+21uu+++/zHTJs2TR06dJDb7ZYkzZo1S8OHD9fixYt1xx13aOvWrdqxY4d2794dgqcIAACiXdBBs3//fo0aNcq/npOTI0nKzs7W2rVr9bvf/U7/93//p0ceeURffvmlBg4cqO3btysuLs5/TFVVlVq1+t/FoYyMDG3cuFFPPfWUnn76aXXr1k2bNm3SwIEDr+S5AQCAFuKKPofmx4TPoUGk8Dk0QGTxOTRm+1F+Dg0AAEAkEDQAAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjETQAAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjETQAAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjhTxounTpIpvN1mCZOXNmo/vv2rWr0f0/+eSTUI8GAACiVEyoT7hv3z5dvHjRv/6vf/1LY8aM0d13393scUeOHFF8fLx//YYbbgj1aAAAIEqFPGi+GyKLFi1St27dNGLEiGaPS0xMVLt27UI9DgAAaAHCeg9NfX291q9fr+nTp8tmszW7b9++feV0OjV69Gjt3LkznGMBAIAoE/IrNN+2ZcsWnTp1Sg8++GCT+zidTq1cuVLp6eny+Xz6wx/+oNGjR2vXrl0aPnx4k8f5fD75fD7/utfrDeXoAADAIGENmlWrVikrK0sul6vJfW666SbddNNN/vXBgwerurpaL774YrNB43a7tWDBgpDOCwAAzBS2XzkdP35cO3bs0K9//eugjx00aJAqKiqa3ScvL08ej8e/VFdXX+6oAADAcGG7QrNmzRolJiZq/PjxQR9bVlYmp9PZ7D52u112u/1yxwMAAFEkLEFz6dIlrVmzRtnZ2YqJCfwReXl5OnHihNatWydJWrJkibp06aKePXv6byIuKChQQUFBOEYDAABRKCxBs2PHDlVVVWn69OkNHqupqVFVVZV/vb6+XnPnztWJEyfUtm1b9ezZU0VFRRo3blw4RgMAAFHIZlmWFekhQsHr9crhcMjj8QR8QB8Qbl1yiyI9AtCiHVsU/K0N+PEI1fs33+UEAACMR9AAAADjETQAAMB4Yf1gPSAY3IsC4HKY+G8H9/2EHldoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPEIGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPEIGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPEIGgAAYLyQB838+fNls9kCluTk5GaPKSkpUXp6umJjY9W1a1etWLEi1GMBAIAoFhOOk/bs2VM7duzwr7du3brJfSsrKzVu3Dg99NBDWr9+vf7xj3/okUce0Q033KBJkyaFYzwAABBlwhI0MTEx33tV5hsrVqxQp06dtGTJEknSzTffrP379+vFF18kaAAAwA8SlntoKioq5HK5lJqaqnvuuUefffZZk/vu3btXmZmZAdvGjh2r/fv36/z58+EYDwAARJmQB83AgQO1bt06/e1vf9Nrr72m2tpaZWRk6Isvvmh0/9raWiUlJQVsS0pK0oULF3Ty5Mkmf47P55PX6w1YAABAyxTyXzllZWX5/7t3794aPHiwunXrptdff105OTmNHmOz2QLWLctqdPu3ud1uLViwIAQTR6cuuUWRHgEA0AQT/40+tmh8pEdoVtj/bPu6665T7969VVFR0ejjycnJqq2tDdhWV1enmJgYtW/fvsnz5uXlyePx+Jfq6uqQzg0AAMwRlpuCv83n8+nw4cMaNmxYo48PHjxYb731VsC27du3q1+/frrmmmuaPK/dbpfdbg/prAAAwEwhv0Izd+5clZSUqLKyUu+//77uuusueb1eZWdnS/r6ysq0adP8+8+YMUPHjx9XTk6ODh8+rNWrV2vVqlWaO3duqEcDAABRKuRXaP7973/r3nvv1cmTJ3XDDTdo0KBBeu+999S5c2dJUk1Njaqqqvz7p6amatu2bZozZ46WLVsml8ulpUuX8ifbAADgB7NZ39yBaziv1yuHwyGPx6P4+PhIjxNxJt5wBgD48QrXTcGhev/mu5wAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPEIGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPEIGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPEIGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGC8kAeN2+1W//79FRcXp8TERE2cOFFHjhxp9phdu3bJZrM1WD755JNQjwcAAKJQyIOmpKREM2fO1Hvvvafi4mJduHBBmZmZOnv27Pcee+TIEdXU1PiXtLS0UI8HAACiUEyoT/jXv/41YH3NmjVKTEzUgQMHNHz48GaPTUxMVLt27UI9EgAAiHJhv4fG4/FIkhISEr533759+8rpdGr06NHauXNns/v6fD55vd6ABQAAtExhDRrLspSTk6OhQ4eqV69eTe7ndDq1cuVKFRQU6M0339RNN92k0aNHq7S0tMlj3G63HA6Hf0lJSQnHUwAAAAawWZZlhevkM2fOVFFRkXbv3q2OHTsGdeyECRNks9lUWFjY6OM+n08+n8+/7vV6lZKSIo/Ho/j4+CuaOxp0yS2K9AgAgChybNH4sJzX6/XK4XBc8ft32K7QPPbYYyosLNTOnTuDjhlJGjRokCoqKpp83G63Kz4+PmABAAAtU8hvCrYsS4899pg2b96sXbt2KTU19bLOU1ZWJqfTGeLpAABANAp50MycOVNvvPGGtm7dqri4ONXW1kqSHA6H2rZtK0nKy8vTiRMntG7dOknSkiVL1KVLF/Xs2VP19fVav369CgoKVFBQEOrxAABAFAp50CxfvlySNHLkyIDta9as0YMPPihJqqmpUVVVlf+x+vp6zZ07VydOnFDbtm3Vs2dPFRUVady4caEeDwAARKGw3hR8NYXqpqJowU3BAIBQarE3BQMAAFwtBA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMR9AAAADjETQAAMB4BA0AADAeQQMAAIxH0AAAAOMRNAAAwHgEDQAAMB5BAwAAjEfQAAAA4xE0AADAeAQNAAAwHkEDAACMFxPpAUzQJbco0iMAAIBmcIUGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPEIGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGC1vQvPrqq0pNTVVsbKzS09P17rvvNrt/SUmJ0tPTFRsbq65du2rFihXhGg0AAESZsATNpk2bNHv2bD355JMqKyvTsGHDlJWVpaqqqkb3r6ys1Lhx4zRs2DCVlZXpiSee0OOPP66CgoJwjAcAAKKMzbIsK9QnHThwoG699VYtX77cv+3mm2/WxIkT5Xa7G+w/b948FRYW6vDhw/5tM2bM0KFDh7R3794f9DO9Xq8cDoc8Ho/i4+Ov/El8S5fcopCeDwAA0xxbND4s5w3V+3dMCGeSJNXX1+vAgQPKzc0N2J6Zmak9e/Y0eszevXuVmZkZsG3s2LFatWqVzp8/r2uuuabBMT6fTz6fz7/u8Xgkff3ChNol31chPycAACYJx/vrt897pddXQh40J0+e1MWLF5WUlBSwPSkpSbW1tY0eU1tb2+j+Fy5c0MmTJ+V0Ohsc43a7tWDBggbbU1JSrmB6AADQGMeS8J7/9OnTcjgcl318yIPmGzabLWDdsqwG275v/8a2fyMvL085OTn+9UuXLum///2v2rdv3+zPCQWv16uUlBRVV1eH/NdbaBqv+9XHax4ZvO5XH695ZHzzun/88cdyuVxXdK6QB83111+v1q1bN7gaU1dX1+AqzDeSk5Mb3T8mJkbt27dv9Bi73S673R6wrV27dpc/+GWIj4/nf/wI4HW/+njNI4PX/erjNY+MDh06qFWrK/s7pZD/lVObNm2Unp6u4uLigO3FxcXKyMho9JjBgwc32H/79u3q169fo/fPAAAAfFtY/mw7JydHv//977V69WodPnxYc+bMUVVVlWbMmCHp618XTZs2zb//jBkzdPz4ceXk5Ojw4cNavXq1Vq1apblz54ZjPAAAEGXCcg/NlClT9MUXX+jZZ59VTU2NevXqpW3btqlz586SpJqamoDPpElNTdW2bds0Z84cLVu2TC6XS0uXLtWkSZPCMd4Vs9vteuaZZxr8ygvhxet+9fGaRwav+9XHax4ZoXzdw/I5NAAAAFcT3+UEAACMR9AAAADjETQAAMB4BA0AADAeQXMZXn31VaWmpio2Nlbp6el69913Iz1S1HK73erfv7/i4uKUmJioiRMn6siRI5Eeq0Vxu92y2WyaPXt2pEeJeidOnNADDzyg9u3b69prr9Utt9yiAwcORHqsqHbhwgU99dRTSk1NVdu2bdW1a1c9++yzunTpUqRHixqlpaWaMGGCXC6XbDabtmzZEvC4ZVmaP3++XC6X2rZtq5EjR+qjjz4K+ucQNEHatGmTZs+erSeffFJlZWUaNmyYsrKyAv4MHaFTUlKimTNn6r333lNxcbEuXLigzMxMnT17NtKjtQj79u3TypUr9dOf/jTSo0S9L7/8UkOGDNE111yjt99+Wx9//LFeeumlq/4J6C3N4sWLtWLFCuXn5+vw4cN6/vnn9cILL+iVV16J9GhR4+zZs+rTp4/y8/Mbffz555/Xyy+/rPz8fO3bt0/JyckaM2aMTp8+HdwPshCUAQMGWDNmzAjY1r17dys3NzdCE7UsdXV1liSrpKQk0qNEvdOnT1tpaWlWcXGxNWLECGvWrFmRHimqzZs3zxo6dGikx2hxxo8fb02fPj1g25133mk98MADEZooukmyNm/e7F+/dOmSlZycbC1atMi/7dy5c5bD4bBWrFgR1Lm5QhOE+vp6HThwQJmZmQHbMzMztWfPnghN1bJ4PB5JUkJCQoQniX4zZ87U+PHj9fOf/zzSo7QIhYWF6tevn+6++24lJiaqb9++eu211yI9VtQbOnSo/v73v+vTTz+VJB06dEi7d+/WuHHjIjxZy1BZWana2tqA91W73a4RI0YE/b4atm/bjkYnT57UxYsXG3zJZlJSUoMv10ToWZalnJwcDR06VL169Yr0OFFt48aNOnjwoPbt2xfpUVqMzz77TMuXL1dOTo6eeOIJffDBB3r88cdlt9sDvioGoTVv3jx5PB51795drVu31sWLF/Xcc8/p3nvvjfRoLcI3752Nva8eP348qHMRNJfBZrMFrFuW1WAbQu/RRx/VP//5T+3evTvSo0S16upqzZo1S9u3b1dsbGykx2kxLl26pH79+mnhwoWSpL59++qjjz7S8uXLCZow2rRpk9avX6833nhDPXv2VHl5uWbPni2Xy6Xs7OxIj9dihOJ9laAJwvXXX6/WrVs3uBpTV1fXoC4RWo899pgKCwtVWlqqjh07RnqcqHbgwAHV1dUpPT3dv+3ixYsqLS1Vfn6+fD6fWrduHcEJo5PT6VSPHj0Ctt18880qKCiI0EQtw29/+1vl5ubqnnvukST17t1bx48fl9vtJmiuguTkZElfX6lxOp3+7Zfzvso9NEFo06aN0tPTVVxcHLC9uLhYGRkZEZoqulmWpUcffVRvvvmm3nnnHaWmpkZ6pKg3evRoffjhhyovL/cv/fr10/3336/y8nJiJkyGDBnS4CMJPv30U/+X+iI8vvrqK7VqFfhW2Lp1a/5s+ypJTU1VcnJywPtqfX29SkpKgn5f5QpNkHJycjR16lT169dPgwcP1sqVK1VVVaUZM2ZEerSoNHPmTL3xxhvaunWr4uLi/FfHHA6H2rZtG+HpolNcXFyDe5Suu+46tW/fnnuXwmjOnDnKyMjQwoULNXnyZH3wwQdauXKlVq5cGenRotqECRP03HPPqVOnTurZs6fKysr08ssva/r06ZEeLWqcOXNGR48e9a9XVlaqvLxcCQkJ6tSpk2bPnq2FCxcqLS1NaWlpWrhwoa699lrdd999wf2gUPwZVkuzbNkyq3PnzlabNm2sW2+9lT8hDiNJjS5r1qyJ9GgtCn+2fXW89dZbVq9evSy73W51797dWrlyZaRHinper9eaNWuW1alTJys2Ntbq2rWr9eSTT1o+ny/So0WNnTt3NvrveHZ2tmVZX//p9jPPPGMlJydbdrvdGj58uPXhhx8G/XNslmVZoSgwAACASOEeGgAAYDyCBgAAGI+gAQAAxiNoAACA8QgaAABgPIIGAAAYj6ABAADGI2gAAIDxCBoAAGA8ggYAABiPoAEAAMYjaAAAgPH+H0B62kiO2PbSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(0)\n",
    "data = np.random.normal(loc=5.0, scale=2.0, size=100)\n",
    "\n",
    "plt.hist(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE via SciPy Optimize\n",
    "\n",
    "Define the negative log‐likelihood for μ and σ (we parametrize via log σ for positivity). We can optimize. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative log-likelihood for Normal\n",
    "def neg_log_lik(params, x):\n",
    "    mu, log_sigma = params\n",
    "    sigma = np.exp(log_sigma)\n",
    "    # sum of −log pdf = negative log-likelihood\n",
    "    return -np.sum(stats.norm.logpdf(x, loc=mu, scale=sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimates:\n",
      "  mu_hat    = 5.11961603106897\n",
      "  sigma_hat = 2.0157644894331592\n"
     ]
    }
   ],
   "source": [
    "# Initial guesses: sample mean & log(sample std)\n",
    "mu0 = np.mean(data)\n",
    "log_sigma0 = np.log(np.std(data, ddof=0)) #(ddof is because we need the population MLE, not the unbiased sample estimator which is N-1 don't worry about this)\n",
    "init = [mu0, log_sigma0]\n",
    "\n",
    "# Optimize\n",
    "res = optimize.minimize(\n",
    "    neg_log_lik,\n",
    "    x0=init,\n",
    "    args=(data,),\n",
    "    method='L-BFGS-B'\n",
    ")\n",
    "\n",
    "mu_hat, sigma_hat = res.x[0], np.exp(res.x[1])\n",
    "print(\"MLE estimates:\")\n",
    "print(\"  mu_hat    =\", mu_hat)\n",
    "print(\"  sigma_hat =\", sigma_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Confidence Intervals\n",
    "\n",
    "Resample the data **with replacement**, refit each bootstrap sample, and then take the 2.5th and 97.5th percentiles of the bootstrap estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CI for μ: [4.723, 5.524]\n",
      "95% CI for sigma: [1.740, 2.251]\n"
     ]
    }
   ],
   "source": [
    "n_boot = 2000\n",
    "mu_boots = []\n",
    "sigma_boots = []\n",
    "\n",
    "for i in range(n_boot):\n",
    "    # resample\n",
    "    samp = np.random.choice(data, size=len(data), replace=True)\n",
    "    # fit\n",
    "    res_i = optimize.minimize(\n",
    "        neg_log_lik,\n",
    "        x0=init,\n",
    "        args=(samp,),\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    mu_i, log_sig_i = res_i.x\n",
    "    mu_boots.append(mu_i)\n",
    "    sigma_boots.append(np.exp(log_sig_i))\n",
    "\n",
    "# Compute percentile CIs\n",
    "alpha = 0.05\n",
    "lower_mu  = np.percentile(mu_boots, 100 * (alpha/2))\n",
    "upper_mu  = np.percentile(mu_boots, 100 * (1 - alpha/2))\n",
    "lower_sig = np.percentile(sigma_boots, 100 * (alpha/2))\n",
    "upper_sig = np.percentile(sigma_boots, 100 * (1 - alpha/2))\n",
    "\n",
    "print(f\"95% CI for μ: [{lower_mu:.3f}, {upper_mu:.3f}]\")\n",
    "print(f\"95% CI for sigma: [{lower_sig:.3f}, {upper_sig:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(\n",
      "    fun,\n",
      "    x0,\n",
      "    args=(),\n",
      "    method=None,\n",
      "    jac=None,\n",
      "    hess=None,\n",
      "    hessp=None,\n",
      "    bounds=None,\n",
      "    constraints=(),\n",
      "    tol=None,\n",
      "    callback=None,\n",
      "    options=None\n",
      ")\n",
      "    Minimization of scalar function of one or more variables.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        The objective function to be minimized::\n",
      "\n",
      "            fun(x, *args) -> float\n",
      "\n",
      "        where ``x`` is a 1-D array with shape (n,) and ``args``\n",
      "        is a tuple of the fixed parameters needed to completely\n",
      "        specify the function.\n",
      "\n",
      "        Suppose the callable has signature ``f0(x, *my_args, **my_kwargs)``, where\n",
      "        ``my_args`` and ``my_kwargs`` are required positional and keyword arguments.\n",
      "        Rather than passing ``f0`` as the callable, wrap it to accept\n",
      "        only ``x``; e.g., pass ``fun=lambda x: f0(x, *my_args, **my_kwargs)`` as the\n",
      "        callable, where ``my_args`` (tuple) and ``my_kwargs`` (dict) have been\n",
      "        gathered before invoking this function.\n",
      "    x0 : ndarray, shape (n,)\n",
      "        Initial guess. Array of real elements of size (n,),\n",
      "        where ``n`` is the number of independent variables.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (`fun`, `jac` and `hess` functions).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "\n",
      "        - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "        - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "        - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "        - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "        - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "        - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "        - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "        - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "        - 'COBYQA'      :ref:`(see here) <optimize.minimize-cobyqa>`\n",
      "        - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "        - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "        - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "        - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "        - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "        - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "        - custom - a callable object, see below for description.\n",
      "\n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending on whether or not the problem has constraints or bounds.\n",
      "    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "        Method for computing the gradient vector. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "        trust-exact and trust-constr.\n",
      "        If it is a callable, it should be a function that returns the gradient\n",
      "        vector::\n",
      "\n",
      "            jac(x, *args) -> array_like, shape (n,)\n",
      "\n",
      "        where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
      "        the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
      "        assumed to return a tuple ``(f, g)`` containing the objective\n",
      "        function and the gradient.\n",
      "        Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
      "        'trust-krylov' require that either a callable be supplied, or that\n",
      "        `fun` return the objective and gradient.\n",
      "        If None or False, the gradient will be estimated using 2-point finite\n",
      "        difference estimation with an absolute step size.\n",
      "        Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
      "        to select a finite difference scheme for numerical estimation of the\n",
      "        gradient with a relative step size. These finite difference schemes\n",
      "        obey any specified `bounds`.\n",
      "    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
      "        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "        trust-ncg, trust-krylov, trust-exact and trust-constr.\n",
      "        If it is callable, it should return the Hessian matrix::\n",
      "\n",
      "            hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)\n",
      "\n",
      "        where ``x`` is a (n,) ndarray and ``args`` is a tuple with the fixed\n",
      "        parameters.\n",
      "        The keywords {'2-point', '3-point', 'cs'} can also be used to select\n",
      "        a finite difference scheme for numerical estimation of the hessian.\n",
      "        Alternatively, objects implementing the `HessianUpdateStrategy`\n",
      "        interface can be used to approximate the Hessian. Available\n",
      "        quasi-Newton methods implementing this interface are:\n",
      "\n",
      "        - `BFGS`\n",
      "        - `SR1`\n",
      "\n",
      "        Not all of the options are available for each of the methods; for\n",
      "        availability refer to the notes.\n",
      "    hessp : callable, optional\n",
      "        Hessian of objective function times an arbitrary vector p. Only for\n",
      "        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "        Only one of `hessp` or `hess` needs to be given. If `hess` is\n",
      "        provided, then `hessp` will be ignored. `hessp` must compute the\n",
      "        Hessian times an arbitrary vector::\n",
      "\n",
      "            hessp(x, p, *args) ->  ndarray shape (n,)\n",
      "\n",
      "        where ``x`` is a (n,) ndarray, ``p`` is an arbitrary vector with\n",
      "        dimension (n,) and ``args`` is a tuple with the fixed\n",
      "        parameters.\n",
      "    bounds : sequence or `Bounds`, optional\n",
      "        Bounds on variables for Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell,\n",
      "        trust-constr, COBYLA, and COBYQA methods. There are two ways to specify\n",
      "        the bounds:\n",
      "\n",
      "        1. Instance of `Bounds` class.\n",
      "        2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "           is used to specify no bound.\n",
      "\n",
      "    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "        Constraints definition. Only for COBYLA, COBYQA, SLSQP and trust-constr.\n",
      "\n",
      "        Constraints for 'trust-constr' and 'cobyqa' are defined as a single object\n",
      "        or a list of objects specifying constraints to the optimization problem.\n",
      "        Available constraints are:\n",
      "\n",
      "        - `LinearConstraint`\n",
      "        - `NonlinearConstraint`\n",
      "\n",
      "        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "        Each dictionary with fields:\n",
      "\n",
      "        type : str\n",
      "            Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "        fun : callable\n",
      "            The function defining the constraint.\n",
      "        jac : callable, optional\n",
      "            The Jacobian of `fun` (only for SLSQP).\n",
      "        args : sequence, optional\n",
      "            Extra arguments to be passed to the function and Jacobian.\n",
      "\n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. When `tol` is specified, the selected\n",
      "        minimization algorithm sets some relevant solver-specific tolerance(s)\n",
      "        equal to `tol`. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods except `TNC` accept the\n",
      "        following generic options:\n",
      "\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform. Depending on the\n",
      "            method each iteration may use several function evaluations.\n",
      "\n",
      "            For `TNC` use `maxfun` instead of `maxiter`.\n",
      "        disp : bool\n",
      "            Set to True to print convergence messages.\n",
      "\n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        A callable called after each iteration.\n",
      "\n",
      "        All methods except TNC, SLSQP, and COBYLA support a callable with\n",
      "        the signature::\n",
      "\n",
      "            callback(intermediate_result: OptimizeResult)\n",
      "\n",
      "        where ``intermediate_result`` is a keyword parameter containing an\n",
      "        `OptimizeResult` with attributes ``x`` and ``fun``, the present values\n",
      "        of the parameter vector and objective function. Note that the name\n",
      "        of the parameter must be ``intermediate_result`` for the callback\n",
      "        to be passed an `OptimizeResult`. These methods will also terminate if\n",
      "        the callback raises ``StopIteration``.\n",
      "\n",
      "        All methods except trust-constr (also) support a signature like::\n",
      "\n",
      "            callback(xk)\n",
      "\n",
      "        where ``xk`` is the current parameter vector.\n",
      "\n",
      "        Introspection is used to determine which of the signatures above to\n",
      "        invoke.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "\n",
      "    **Unconstrained minimization**\n",
      "\n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
      "    first derivatives are used.\n",
      "\n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "\n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm. Suitable for large-scale\n",
      "    problems.\n",
      "\n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "\n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "\n",
      "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "    minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    On indefinite problems it requires usually less iterations than the\n",
      "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "\n",
      "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "    is a trust-region method for unconstrained minimization in which\n",
      "    quadratic subproblems are solved almost exactly [13]_. This\n",
      "    algorithm requires the gradient and the Hessian (which is\n",
      "    *not* required to be positive definite). It is, in many\n",
      "    situations, the Newton method to converge in fewer iterations\n",
      "    and the most recommended for small and medium-size problems.\n",
      "\n",
      "    **Bound-Constrained minimization**\n",
      "\n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "\n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "\n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken. If bounds are not provided, then an\n",
      "    unbounded line search will be used. If bounds are provided and\n",
      "    the initial guess is within the bounds, then every function\n",
      "    evaluation throughout the minimization procedure will be within\n",
      "    the bounds. If bounds are provided, the initial guess is outside\n",
      "    the bounds, and `direc` is full rank (default has full rank), then\n",
      "    some function evaluations during the first iteration may be\n",
      "    outside the bounds, but every function evaluation after the first\n",
      "    iteration will be within the bounds. If `direc` is not full rank,\n",
      "    then some parameters may not be optimized and the solution is not\n",
      "    guaranteed to be within the bounds.\n",
      "\n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "\n",
      "    **Constrained Minimization**\n",
      "\n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "\n",
      "    Method :ref:`COBYQA <optimize.minimize-cobyqa>` uses the Constrained\n",
      "    Optimization BY Quadratic Approximations (COBYQA) method [18]_. The\n",
      "    algorithm is a derivative-free trust-region SQP method based on quadratic\n",
      "    approximations to the objective function and each nonlinear constraint. The\n",
      "    bounds are treated as unrelaxable constraints, in the sense that the\n",
      "    algorithm always respects them throughout the optimization process.\n",
      "\n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "\n",
      "    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "    trust-region algorithm for constrained optimization. It switches\n",
      "    between two implementations depending on the problem definition.\n",
      "    It is the most versatile constrained minimization algorithm\n",
      "    implemented in SciPy and the most appropriate for large-scale problems.\n",
      "    For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "    inequality constraints are imposed as well, it switches to the trust-region\n",
      "    interior point method described in [16]_. This interior point algorithm,\n",
      "    in turn, solves inequality constraints by introducing slack variables\n",
      "    and solving a sequence of equality-constrained barrier problems\n",
      "    for progressively smaller values of the barrier parameter.\n",
      "    The previously described equality constrained SQP method is\n",
      "    used to solve the subproblems with increasing levels of accuracy\n",
      "    as the iterate gets closer to a solution.\n",
      "\n",
      "    **Finite-Difference Options**\n",
      "\n",
      "    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "    the gradient and the Hessian may be approximated using\n",
      "    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "    The scheme 'cs' is, potentially, the most accurate but it\n",
      "    requires the function to correctly handle complex inputs and to\n",
      "    be differentiable in the complex plane. The scheme '3-point' is more\n",
      "    accurate than '2-point' but requires twice as many operations. If the\n",
      "    gradient is estimated via finite-differences the Hessian must be\n",
      "    estimated using one of the quasi-Newton strategies.\n",
      "\n",
      "    **Method specific options for the** `hess` **keyword**\n",
      "\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | method/Hess  | None | callable | '2-point/'3-point'/'cs' | HUS |\n",
      "    +==============+======+==========+=========================+=====+\n",
      "    | Newton-CG    | x    | (n, n)   | x                       | x   |\n",
      "    |              |      | LO       |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | dogleg       |      | (n, n)   |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-ncg    |      | (n, n)   | x                       | x   |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-krylov |      | (n, n)   | x                       | x   |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-exact  |      | (n, n)   |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-constr | x    | (n, n)   |  x                      | x   |\n",
      "    |              |      | LO       |                         |     |\n",
      "    |              |      | sp       |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "\n",
      "    where LO=LinearOperator, sp=Sparse matrix, HUS=HessianUpdateStrategy\n",
      "\n",
      "    **Custom minimizers**\n",
      "\n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "\n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "    object.\n",
      "\n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "       Trust region methods. 2000. Siam. pp. 169-200.\n",
      "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "       implementation of the GLTR method for iterative solution of\n",
      "       the trust region problem\", :arxiv:`1611.04718`\n",
      "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "       Trust-Region Subproblem using the Lanczos Method\",\n",
      "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "        An interior point algorithm for large-scale nonlinear  programming.\n",
      "        SIAM Journal on Optimization 9.4: 877-900.\n",
      "    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. 1998. On the\n",
      "        implementation of an algorithm for large-scale equality constrained\n",
      "        optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "    .. [18] Ragonneau, T. M. *Model-Based Derivative-Free Optimization Methods\n",
      "        and Software*. PhD thesis, Department of Applied Mathematics, The Hong\n",
      "        Kong Polytechnic University, Hong Kong, China, 2022. URL:\n",
      "        https://theses.lib.polyu.edu.hk/handle/200/12294.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "\n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "\n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "\n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "\n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([\n",
      "        [ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "        [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "        [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "        [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "        [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]\n",
      "    ])\n",
      "\n",
      "\n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "\n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "\n",
      "    There are three constraints defined as:\n",
      "\n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "\n",
      "    And variables must be positive, hence the following bounds:\n",
      "\n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "\n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "\n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "\n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimize.minimize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
